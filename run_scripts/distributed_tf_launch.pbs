#!/bin/bash

#PBS -l nodes=16:ppn=1:xk
#PBS -l walltime=02:00:00
#PBS -N distributed_tf_launch
#PBS -e logs/log.${PBS_JOBNAME}_${PBS_JOBID}.err
#PBS -o logs/log.${PBS_JOBNAME}_${PBS_JOBID}.out

## For more options, consult https://bluewaters.ncsa.illinois.edu/batch-jobs

cd $PBS_O_WORKDIR

module load bwpy/0.3.1

NUM_BATCHES=100
DATA_DIR="${HOME}/scratch/ImageNet/tf_records"
TRAIN_DIR="train_dir" # Directory where TensorFlow saves graphs and checkpoints
NUM_PS_HOSTS=1

DO_TRAIN_VAL="--data_dir $DATA_DIR/train"
#DO_TRAIN_VAL="--data_dir $DATA_DIR/validation --eval"

HOST_NAMES=$(aprun -q -n $PBS_NUM_NODES -N 1 -- hostname)
let PS_HOST_COUNT=0
let WORKER_HOST_COUNT=0

PS_HOSTS_TASKS=""
WORKER_HOSTS_TASKS=""

for host_name in $HOST_NAMES
do
    if [ $PS_HOST_COUNT -lt $NUM_PS_HOSTS ]
    then
	PS_HOSTS_TASKS="$host_name:$PS_HOST_COUNT,$PS_HOSTS_TASKS" # , and : are delimiters
	let PS_HOST_COUNT++
    else
	WORKER_HOSTS_TASKS="$host_name:$WORKER_HOST_COUNT,$WORKER_HOSTS_TASKS" # , and : are delimiters
	let WORKER_HOST_COUNT++
    fi
done

WORKER_HOSTS_TASKS=$(sed 's|,$||' <<< $WORKER_HOSTS_TASKS)
PS_HOSTS_TASKS=$(sed 's|,$||' <<< $PS_HOSTS_TASKS)

RUN_CMD="${PBS_O_WORKDIR}/dist.sh"
# parameter_server \
# --variable_update distributed_replicated \
# --graph_file ${TRAIN_DIR}/def_graph.pb \
# --sync_on_finish

ADDITIONAL_ARGS="--batch_size 32 \
--num_warmup_batches 0 \
--num_batches ${NUM_BATCHES} \
--model inception3 \
--data_name imagenet \
--variable_update parameter_server \
--train_dir ${TRAIN_DIR} \
--eval_dir ${TRAIN_DIR}/eval \
--num_intra_threads 1 \
--num_inter_threads 0"

RUN_ARGUMENTS="${WORKER_HOSTS_TASKS} ${PS_HOSTS_TASKS} ${DATA_DIR} ${TRAIN_DIR} ${ADDITIONAL_ARGS} ${DO_TRAIN_VAL}"

echo "Running $RUN_CMD $RUN_ARGUMENTS"

aprun -b -n ${PBS_NUM_NODES} -N ${PBS_NUM_PPN} -- $RUN_CMD $RUN_ARGUMENTS \
    1> $PBS_O_WORKDIR/logs/${PBS_JOBNAME}_${PBS_JOBID}.out \
    2> $PBS_O_WORKDIR/logs/${PBS_JOBNAME}_${PBS_JOBID}.err
echo "Done, thank you for flying."
