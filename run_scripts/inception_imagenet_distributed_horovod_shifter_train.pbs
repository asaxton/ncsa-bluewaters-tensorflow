#!/bin/bash
#PBS -l nodes=8:ppn=16:xk
#PBS -l walltime=48:00:00
#PBS -N horovod_distributed_train
#PBS -M saxton@illinois.edu
#PBS -l advres=banw1
#PBS -A banw
#PBS -l flags=commtransparent
#PBS -l gres=shifter16
#PBS -e logs/log.${PBS_JOBNAME}_NN_${PBS_NUM_NODES}_${PBS_JOBID}.err
#PBS -o logs/log.${PBS_JOBNAME}_NN_${PBS_NUM_NODES}_${PBS_JOBID}.out
##PBS -v UDI="rhaas/tensorflow:16.04f -v /mnt/b/projects/sciteam/banw/tensorflow:/work -v /dsl/opt/cray/:/opt/cray -v /dsl/opt/gcc:/opt/gcc -v /dsl/opt/nvidia:/opt/nvidia -v /projects/sciteam/banw/white_rabbit/ncsa-bluewaters-tensorflow:/Proj -v /mnt/c/scratch/staff/saxton/ImageNet2/tf_records/train:/data_dir -v /projects/sciteam/banw/white_rabbit/ncsa-bluewaters-tensorflow:/checkpoints"

#PBS -v UDI="rhaas/tensorflow:16.04f -v /mnt/b/projects/sciteam/banw/tensorflow:/work -v /dsl/opt/cray/:/opt/cray -v /dsl/opt/gcc:/opt/gcc -v /dsl/opt/nvidia:/opt/nvidia -v /mnt/b/projects/sciteam/banw/white_rabbit/ncsa-bluewaters-tensorflow:/Proj -v /mnt/c/scratch/staff/saxton/ImageNet2/tf_records/train:/data_dir -v /mnt/b/projects/sciteam/banw/white_rabbit/ncsa-bluewaters-tensorflow:/checkpoints"

# NOTE: lines that begin with "#PBS" are not interpreted by the shell but ARE 
# used by the batch system, wheras lines that begin with multiple # signs, 
# like "##PBS" are considered "commented out" by the batch system 
# and have no effect.  

# If you launched the job in a directory prepared for the job to run within, 
# you'll want to cd to that directory
# [uncomment the following line to enable this]
cd $PBS_O_WORKDIR

# Alternatively, the job script can create its own job-ID-unique directory 
# to run within.  In that case you'll need to create and populate that 
# directory with executables and perhaps inputs
# [uncomment and customize the following lines to enable this behavior] 
# mkdir -p /scratch/sciteam/$USER/$PBS_JOBID
# cd /scratch/sciteam/$USER/$PBS_JOBID
# cp /scratch/job/setup/directory/* .

# To add certain modules that you do not have added via ~/.modules 
#. /opt/modules/default/init/bash
#module load craype-hugepages2M  perftools

### launch the application
### redirecting stdin and stdout if needed
### NOTE: (the "in" file must exist for input)

#export CRAY_CUDA_MPS=1
#export CUDA_VISIBLE_DEVICES=-1
#export OMP_NUM_THREADS=4
echo "Starting"
mkdir -p logs

PROJ_DIR='/mnt/a/u/staff/saxton/Development/bw_tf_dev_0.0.1/ncsa-bluewaters-tensorflow'
WORK_DIR='/mnt/b/projects/sciteam/banw/tensorflow'
. /opt/modules/default/init/bash

# get access to MPI libraries and stuff
module unload PrgEnv-cray
module load PrgEnv-gnu
module unload cray-mpich
module load cray-mpich-abi
# need to remove gcc to avoid messing with LD_LIBRARY_PATH's C++ library
module unload gcc

module load shifter/16.08.3-1.0502.8871-cray_gem
module load cudatoolkit

export CUDA_VISIBLE_DEVICES=0
export CRAY_ROOTFS=SHIFTER
export LD_LIBRARY_PATH=/work/tf/mpi:$(readlink -f /opt/cray/wlm_detect/default/lib64):$(readlink -f /opt/cray/nvidia/default/lib64):$LD_LIBRARY_PATH:$CRAY_LD_LIBRARY_PATH:/usr/local/cuda/lib64
NUM_SAMPLES=385455
EPOCHS=100
#export NUM_STEPS=300000
export NUM_STEPS=$((EPOCHS*NUM_SAMPLES))
DATA_DIR="/mnt/c/scratch/staff/saxton/ImageNet2/tf_records/train" #"${HOME}/scratch/ImageNet2/tf_records"

BATCH_SIZE=32

export MPICH_MAX_THREAD_SAFETY=multiple
#NODES_USED=2
for NODES_USED in $((PBS_NUM_NODES - 6)) $((PBS_NUM_NODES - 5)) $((PBS_NUM_NODES - 4)) $((PBS_NUM_NODES - 3)) $((PBS_NUM_NODES - 2)); do

RUN_CMD="python /Proj/BWDistributedTrain/inception_imagenet_horovod_distributed_train.py \
--data_dir /data_dir \
--num_steps $NUM_STEPS \
--num_train_examples $NUM_SAMPLES \
--batch_size ${BATCH_SIZE} \
--initial_learning_rate 0.1 \
--checkpoint_dir /checkpoints/checkpoint_dir_${PBS_JOBNAME}_NU_${NODES_USED}_${PBS_JOBID}"

aprun -cc none -n ${NODES_USED} -d 16 -N 1 -b -- bash -c "source /work/tf/bin/activate; export LD_LIBRARY_PATH=${LD_LIBRARY_PATH} ; ${RUN_CMD}"

if [ $? -eq 0 ] ; then break ; fi
done

echo "Done, Thank you for flying."

